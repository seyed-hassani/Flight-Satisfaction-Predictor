{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f59bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras --upgrade\n",
    "# uncomment the above line if you don't have keras 3 installed\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04dae5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv',engine='pyarrow')\n",
    "test = pd.read_csv('test.csv',engine ='pyarrow')\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "786411f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train= train.drop(['Unnamed: 0','id'],axis=1)\n",
    "test= test.drop(['Unnamed: 0','id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88d6be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.isna().any()\n",
    "train['Arrival Delay in Minutes'].fillna(train['Arrival Delay in Minutes'].mean(), inplace=True)\n",
    "\n",
    "test['Arrival Delay in Minutes'].fillna(test['Arrival Delay in Minutes'].mean(), inplace=True)\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0a8c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "\n",
    "# normilzation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "num_cols = [\n",
    "    'Age', 'Flight Distance',\n",
    "    'Departure Delay in Minutes',\n",
    "    'Arrival Delay in Minutes'\n",
    "]\n",
    "\n",
    "# مقیاس‌دهی 0 تا 1\n",
    "scaler = MinMaxScaler()\n",
    "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "test[num_cols] = scaler.transform(test[num_cols])\n",
    "# 1- Service Quality Score\n",
    "train['service_score'] = (\n",
    "    train['Inflight entertainment'] +\n",
    "    train['Seat comfort'] +\n",
    "    train['On-board service'] +\n",
    "    train['Leg room service'] +\n",
    "    train['Cleanliness'] +\n",
    "    train['Inflight wifi service'] +\n",
    "    train['Inflight service'] +\n",
    "    train['Checkin service'] +\n",
    "    train['Food and drink']\n",
    ")\n",
    "test['service_score'] = (\n",
    "    test['Inflight entertainment'] +\n",
    "    test['Seat comfort'] +\n",
    "    test['On-board service'] +\n",
    "    test['Leg room service'] +\n",
    "    test['Cleanliness'] +\n",
    "    test['Inflight wifi service'] +\n",
    "    test['Inflight service'] +\n",
    "    test['Checkin service'] +\n",
    "    test['Food and drink']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa65058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3- Travel Type\n",
    "train['Travel_Class'] = train['Type of Travel']  + train['Class']\n",
    "test['Travel_Class'] = test['Type of Travel']  + test['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52ce860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4- Loyalty or New Customer\n",
    "train['Is_New_Customer'] = (train['Customer Type'] == 'disloyal Customer').astype(int)\n",
    "test['Is_New_Customer'] = (test['Customer Type'] == 'disloyal Customer').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6349873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5- Flight Duration\n",
    "train['Flight Category'] = pd.cut(train['Flight Distance'], bins=[0, 500, 1500, 3000, float('inf')],\n",
    "                               labels=['Short', 'Medium', 'Long', 'Very Long'])\n",
    "test['Flight Category'] = pd.cut(test['Flight Distance'], bins=[0, 500, 1500, 3000, float('inf')],\n",
    "                               labels=['Short', 'Medium', 'Long', 'Very Long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f30fe901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "\n",
    "# فقط ستون‌های مؤثر رو نگه می‌داریم:\n",
    "useful_features = [\n",
    "    'Online boarding',\n",
    "    'Inflight entertainment',\n",
    "    'Seat comfort',\n",
    "    'On-board service',\n",
    "    'Leg room service',\n",
    "    'Cleanliness',\n",
    "    'Flight Distance',\n",
    "    'Inflight wifi service',\n",
    "    'Baggage handling',\n",
    "    'Inflight service',\n",
    "    'Checkin service',\n",
    "    'Food and drink',\n",
    "    'Ease of Online booking',\n",
    "    'Age',\n",
    "    'service_score',\n",
    "    'Travel_Class',\n",
    "    'Is_New_Customer',\n",
    "    'Flight Category'\n",
    "]\n",
    "\n",
    "# به‌علاوه ستون‌های دسته‌ای که باید تبدیل بشن:\n",
    "categorical_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class']\n",
    "\n",
    "# فقط ستون‌های موردنیاز رو برمی‌داریم\n",
    "train = train[useful_features + categorical_cols + ['satisfaction']]\n",
    "test = test[useful_features + categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f94d59dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['satisfaction'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db4b3d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add target back to the feature DataFrame temporarily\n",
    "df_corr = train.copy()\n",
    "df_corr['satisfaction_encoded'] = LabelEncoder().fit_transform(df_corr['satisfaction'])\n",
    "\n",
    "# Get numeric correlation\n",
    "corr = df_corr.corr(numeric_only=True)\n",
    "\n",
    "# Correlation with the target\n",
    "target_corr = corr['satisfaction_encoded'].sort_values(ascending=False)\n",
    "\n",
    "# Display the most and least related features\n",
    "print(target_corr)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=target_corr.values, y=target_corr.index)\n",
    "plt.title(\"Feature Correlation with Satisfaction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e5befb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns = train.drop('satisfaction', axis=1).select_dtypes(include=['object', 'category']).columns\n",
    "print(dummy_columns)\n",
    "\n",
    "\n",
    "train = pd.get_dummies(train, columns=dummy_columns,dtype=np.uint8)\n",
    "test = pd.get_dummies(test, columns=dummy_columns,dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26c51c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make Validation Set (Optional)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train.drop('satisfaction', axis=1),  # drop the column by name\n",
    "    train['satisfaction'],               # target variable\n",
    "    test_size=0.3,\n",
    "    random_state=42                     # optional: for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c21efe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18fc5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 0.0003\n",
    "L2_REG = 1e-5\n",
    "# Reduced dropout rates (tuned based on experience)\n",
    "DROPOUT_RATES = [0.3, 0.2, 0.1, 0.05]\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "# --- Assumption: X_train, y_train, X_valid, y_valid are already defined ---\n",
    "# For example, these could come from your preprocessed and split data:\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# --- Model Definition ---\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "x = Dense(384, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(DROPOUT_RATES[0])(x)\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(DROPOUT_RATES[1])(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(DROPOUT_RATES[2])(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(DROPOUT_RATES[3])(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# --- Compile Model ---\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# --- Callbacks ---\n",
    "checkpoint_cb = ModelCheckpoint(\"model_962.keras\",\n",
    "                                  save_best_only=True,\n",
    "                                  monitor='val_accuracy',  # Monitor accuracy improvements\n",
    "                                  mode='max',\n",
    "                                  verbose=1)\n",
    "earlystop_cb = EarlyStopping(patience=10,\n",
    "                             restore_best_weights=True,\n",
    "                             monitor='val_accuracy',\n",
    "                             mode='max',\n",
    "                             verbose=1)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                 patience=3,\n",
    "                                 factor=0.5,\n",
    "                                 min_lr=1e-6,\n",
    "                                 verbose=1)\n",
    "\n",
    "# --- Train the Model ---\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9cfe190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# --- Example: Load your data and perform train/test split ---\n",
    "# Assumed that you have a dataset and you split it into features (X) and target (y).\n",
    "# Replace the following placeholders with your actual data loading/preprocessing.\n",
    "# For example:\n",
    "# data = pd.read_csv(\"your_data.csv\")\n",
    "# X = data.drop(\"target\", axis=1).values\n",
    "# y = data[\"target\"].values\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Example placeholder arrays (replace these with your actual preprocessed arrays):\n",
    "X_train = np.random.rand(800, 20)  # placeholder: 800 samples, 20 features\n",
    "X_valid = np.random.rand(200, 20)\n",
    "y_train = np.random.randint(0, 2, 800)  # binary target\n",
    "y_valid = np.random.randint(0, 2, 200)\n",
    "\n",
    "# --- Ensure data is numeric (convert to float32) ---\n",
    "X_train = np.asarray(X_train).astype('float32')\n",
    "X_valid = np.asarray(X_valid).astype('float32')\n",
    "# For binary crossentropy, it's a good idea to work in float32 even for targets\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_valid = np.asarray(y_valid).astype('float32')\n",
    "\n",
    "# --- Revised Model Definition with L2 Regularization and Adjusted Dropout ---\n",
    "LEARNING_RATE = 0.0003\n",
    "L2_REG = 1e-5\n",
    "# Adjusted dropout rates for improved performance\n",
    "DROPOUT_RATES = [0.3, 0.2, 0.1, 0.05]\n",
    "\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "x = Dense(384, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(DROPOUT_RATES[0])(x)\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(DROPOUT_RATES[1])(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(DROPOUT_RATES[2])(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(L2_REG))(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(DROPOUT_RATES[3])(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# --- Compile Model ---\n",
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# --- Define Callbacks ---\n",
    "checkpoint_cb = ModelCheckpoint(\"model_962.keras\", save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
    "earlystop_cb = EarlyStopping(patience=7, restore_best_weights=True, monitor='val_accuracy', mode='max')\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# --- Train ---\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "518743bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Evaluate\n",
    "y_pred_probs = model.predict(X_valid)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Get model predictions (as probabilities)\n",
    "y_val_pred_probs = model.predict(X_valid)\n",
    "\n",
    "# Convert probabilities to class labels (0 or 1)\n",
    "y_val_pred = (y_val_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_valid, y_val_pred)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Step 1: Predict (assumes X_test is preprocessed)\n",
    "y_test_pred_probs = model.predict(test)\n",
    "y_test_pred = (y_test_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Step 2: Map 0/1 to correct labels\n",
    "label_map = {0: 'neutral or dissatisfied', 1: 'satisfied'}\n",
    "y_test_labels = [label_map[val] for val in y_test_pred]\n",
    "print(y_test_labels)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a78193a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Model Definition\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "x = Dense(384, kernel_initializer='he_normal')(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"model_962.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=7, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faabe76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Evaluate\n",
    "y_pred_probs = model.predict(X_valid)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Get model predictions (as probabilities)\n",
    "y_val_pred_probs = model.predict(X_valid)\n",
    "\n",
    "# Convert probabilities to class labels (0 or 1)\n",
    "y_val_pred = (y_val_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_valid, y_val_pred)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Step 1: Predict (assumes X_test is preprocessed)\n",
    "y_test_pred_probs = model.predict(test)\n",
    "y_test_pred = (y_test_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Step 2: Map 0/1 to correct labels\n",
    "label_map = {0: 'neutral or dissatisfied', 1: 'satisfied'}\n",
    "y_test_labels = [label_map[val] for val in y_test_pred]\n",
    "print(y_test_labels)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d859aef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Model Definition (2 layers)\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# First hidden layer\n",
    "x = Dense(128, kernel_initializer='he_normal')(inputs)  # Adjust number of neurons as needed\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)  # Adjust dropout rate as needed\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(1, activation='sigmoid')(x)  \n",
    "\n",
    "# Create the model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks (remain the same)\n",
    "checkpoint_cb = ModelCheckpoint(\"model_962.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=7, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Train (remain the same)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b39c9a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# Evaluate\n",
    "y_pred_probs = model.predict(X_valid)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Get model predictions (as probabilities)\n",
    "y_val_pred_probs = model.predict(X_valid)\n",
    "\n",
    "# Convert probabilities to class labels (0 or 1)\n",
    "y_val_pred = (y_val_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_valid, y_val_pred)\n",
    "\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Step 1: Predict (assumes X_test is preprocessed)\n",
    "y_test_pred_probs = model.predict(test)\n",
    "y_test_pred = (y_test_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Step 2: Map 0/1 to correct labels\n",
    "label_map = {0: 'neutral or dissatisfied', 1: 'satisfied'}\n",
    "y_test_labels = [label_map[val] for val in y_test_pred]\n",
    "print(y_test_labels)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46ec94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Model Definition (2 layers with potential improvements)\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# First hidden layer (with L2 regularization)\n",
    "x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=l2(0.001))(inputs)  \n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x) \n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(1, activation='sigmoid')(x)  \n",
    "\n",
    "# Create the model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile (with a lower learning rate)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)  \n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks (with increased patience)\n",
    "checkpoint_cb = ModelCheckpoint(\"model_962.keras\", save_best_only=True, monitor='val_accuracy') \n",
    "earlystop_cb = EarlyStopping(patience=10, restore_best_weights=True, monitor='val_accuracy')  \n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_accuracy', patience=5, factor=0.5, min_lr=1e-6, verbose=1) \n",
    "\n",
    "# Train (with more epochs if needed)\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=200,  \n",
    "    batch_size=64,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91872543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Step 1: Encode categorical features\n",
    "X = pd.get_dummies(train.drop(columns=['satisfaction']), drop_first=True)\n",
    "\n",
    "# Step 2: Encode target\n",
    "y = LabelEncoder().fit_transform(train['satisfaction'])  # 0: neutral/dissatisfied, 1: satisfied\n",
    "\n",
    "# Step 3: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 4: Split the data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 5: Define the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal')(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25d1a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Step 1: Encode categorical features\n",
    "X = pd.get_dummies(train.drop(columns=['satisfaction']), drop_first=True)\n",
    "\n",
    "# Step 2: Encode target\n",
    "y = LabelEncoder().fit_transform(train['satisfaction'])  # 0: neutral/dissatisfied, 1: satisfied\n",
    "\n",
    "# Step 3: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 4: Split the data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 5: Define the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal')(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Reduced learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=150,  # Increased epochs\n",
    "    batch_size=32,  # Reduced batch size\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "918b6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras --upgrade\n",
    "# uncomment the above line if you don't have keras 3 installed\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e69c280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv',engine='pyarrow')\n",
    "test = pd.read_csv('test.csv',engine ='pyarrow')\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4fc52a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train= train.drop(['Unnamed: 0','id'],axis=1)\n",
    "test= test.drop(['Unnamed: 0','id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d960d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.isna().any()\n",
    "train['Arrival Delay in Minutes'].fillna(train['Arrival Delay in Minutes'].mean(), inplace=True)\n",
    "\n",
    "test['Arrival Delay in Minutes'].fillna(test['Arrival Delay in Minutes'].mean(), inplace=True)\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33a3f8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "\n",
    "# normilzation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "num_cols = [\n",
    "    'Age', 'Flight Distance',\n",
    "    'Departure Delay in Minutes',\n",
    "    'Arrival Delay in Minutes'\n",
    "]\n",
    "\n",
    "# مقیاس‌دهی 0 تا 1\n",
    "scaler = MinMaxScaler()\n",
    "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "test[num_cols] = scaler.transform(test[num_cols])\n",
    "# 1- Service Quality Score\n",
    "train['service_score'] = (\n",
    "    train['Inflight entertainment'] +\n",
    "    train['Seat comfort'] +\n",
    "    train['On-board service'] +\n",
    "    train['Leg room service'] +\n",
    "    train['Cleanliness'] +\n",
    "    train['Inflight wifi service'] +\n",
    "    train['Inflight service'] +\n",
    "    train['Checkin service'] +\n",
    "    train['Food and drink']\n",
    ")\n",
    "test['service_score'] = (\n",
    "    test['Inflight entertainment'] +\n",
    "    test['Seat comfort'] +\n",
    "    test['On-board service'] +\n",
    "    test['Leg room service'] +\n",
    "    test['Cleanliness'] +\n",
    "    test['Inflight wifi service'] +\n",
    "    test['Inflight service'] +\n",
    "    test['Checkin service'] +\n",
    "    test['Food and drink']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8235edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-Total Delay\n",
    "train['Total Delay'] = train['Departure Delay in Minutes'] + train['Arrival Delay in Minutes']\n",
    "train['Delay Category'] = pd.cut(train['Total Delay'], bins=[-1, 0, 15, 60, 180, float('inf')],\n",
    "                               labels=['No delay', 'Short', 'Moderate', 'Long', 'Extreme'])\n",
    "\n",
    "#2-Total Delay\n",
    "test['Total Delay'] = test['Departure Delay in Minutes'] + test['Arrival Delay in Minutes']\n",
    "test['Delay Category'] = pd.cut(test['Total Delay'], bins=[-1, 0, 15, 60, 180, float('inf')],\n",
    "                               labels=['No delay', 'Short', 'Moderate', 'Long', 'Extreme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3c02a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3- Travel Type\n",
    "train['Travel_Class'] = train['Type of Travel']  + train['Class']\n",
    "test['Travel_Class'] = test['Type of Travel']  + test['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3773f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4- Loyalty or New Customer\n",
    "train['Is_New_Customer'] = (train['Customer Type'] == 'disloyal Customer').astype(int)\n",
    "test['Is_New_Customer'] = (test['Customer Type'] == 'disloyal Customer').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24a08077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5- Flight Duration\n",
    "train['Flight Category'] = pd.cut(train['Flight Distance'], bins=[0, 500, 1500, 3000, float('inf')],\n",
    "                               labels=['Short', 'Medium', 'Long', 'Very Long'])\n",
    "test['Flight Category'] = pd.cut(test['Flight Distance'], bins=[0, 500, 1500, 3000, float('inf')],\n",
    "                               labels=['Short', 'Medium', 'Long', 'Very Long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97fcf955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "\n",
    "# فقط ستون‌های مؤثر رو نگه می‌داریم:\n",
    "useful_features = [\n",
    "    'Online boarding',\n",
    "    'Inflight entertainment',\n",
    "    'Seat comfort',\n",
    "    'On-board service',\n",
    "    'Leg room service',\n",
    "    'Cleanliness',\n",
    "    'Flight Distance',\n",
    "    'Inflight wifi service',\n",
    "    'Baggage handling',\n",
    "    'Inflight service',\n",
    "    'Checkin service',\n",
    "    'Food and drink',\n",
    "    'Ease of Online booking',\n",
    "    'Age',\n",
    "    'service_score',\n",
    "    'Travel_Class',\n",
    "    'Is_New_Customer',\n",
    "    'Flight Category'\n",
    "]\n",
    "\n",
    "# به‌علاوه ستون‌های دسته‌ای که باید تبدیل بشن:\n",
    "categorical_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class']\n",
    "\n",
    "# فقط ستون‌های موردنیاز رو برمی‌داریم\n",
    "train = train[useful_features + categorical_cols + ['satisfaction']]\n",
    "test = test[useful_features + categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ddd245a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['satisfaction'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c62b5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add target back to the feature DataFrame temporarily\n",
    "df_corr = train.copy()\n",
    "df_corr['satisfaction_encoded'] = LabelEncoder().fit_transform(df_corr['satisfaction'])\n",
    "\n",
    "# Get numeric correlation\n",
    "corr = df_corr.corr(numeric_only=True)\n",
    "\n",
    "# Correlation with the target\n",
    "target_corr = corr['satisfaction_encoded'].sort_values(ascending=False)\n",
    "\n",
    "# Display the most and least related features\n",
    "print(target_corr)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=target_corr.values, y=target_corr.index)\n",
    "plt.title(\"Feature Correlation with Satisfaction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75c206c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns = train.drop('satisfaction', axis=1).select_dtypes(include=['object', 'category']).columns\n",
    "print(dummy_columns)\n",
    "\n",
    "\n",
    "train = pd.get_dummies(train, columns=dummy_columns,dtype=np.uint8)\n",
    "test = pd.get_dummies(test, columns=dummy_columns,dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28d630f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make Validation Set (Optional)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train.drop('satisfaction', axis=1),  # drop the column by name\n",
    "    train['satisfaction'],               # target variable\n",
    "    test_size=0.3,\n",
    "    random_state=42                     # optional: for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eae2248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16792f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Step 1: Encode categorical features\n",
    "X = pd.get_dummies(train.drop(columns=['satisfaction']), drop_first=True)\n",
    "\n",
    "# Step 2: Encode target\n",
    "y = LabelEncoder().fit_transform(train['satisfaction'])  # 0: neutral/dissatisfied, 1: satisfied\n",
    "\n",
    "# Step 3: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 4: Split the data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 5: Define the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal')(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Reduced learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=150,  # Increased epochs\n",
    "    batch_size=32,  # Reduced batch size\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "016f3bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Step 1: Encode categorical features\n",
    "X = pd.get_dummies(train.drop(columns=['satisfaction']), drop_first=True)\n",
    "\n",
    "# Step 2: Encode target\n",
    "y = LabelEncoder().fit_transform(train['satisfaction'])  # 0: neutral/dissatisfied, 1: satisfied\n",
    "\n",
    "# Step 3: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 4: Split the data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 5: Define the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "L2_REG = 1e-5\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Reduced learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=150,  # Increased epochs\n",
    "    batch_size=32,  # Reduced batch size\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d694b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Step 5: Define the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "L2_REG = 1e-5\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal', kernel_regularizer=regularizers.l2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Reduced learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=150,  # Increased epochs\n",
    "    batch_size=32,  # Reduced batch size\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4315ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Step 5: Define the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "L2_REG = 1e-5\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Reduced learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=150,  # Increased epochs\n",
    "    batch_size=32,  # Reduced batch size\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0537a33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras --upgrade\n",
    "# uncomment the above line if you don't have keras 3 installed\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1cee95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv',engine='pyarrow')\n",
    "test = pd.read_csv('test.csv',engine ='pyarrow')\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5c6ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train= train.drop(['Unnamed: 0','id'],axis=1)\n",
    "test= test.drop(['Unnamed: 0','id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5482b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.isna().any()\n",
    "train['Arrival Delay in Minutes'].fillna(train['Arrival Delay in Minutes'].mean(), inplace=True)\n",
    "\n",
    "test['Arrival Delay in Minutes'].fillna(test['Arrival Delay in Minutes'].mean(), inplace=True)\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3661493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "\n",
    "# فقط ستون‌های مؤثر رو نگه می‌داریم:\n",
    "useful_features = [\n",
    "    'Online boarding',\n",
    "    'Inflight entertainment',\n",
    "    'Seat comfort',\n",
    "    'On-board service',\n",
    "    'Leg room service',\n",
    "    'Cleanliness',\n",
    "    'Flight Distance',\n",
    "    'Inflight wifi service',\n",
    "    'Baggage handling',\n",
    "    'Inflight service',\n",
    "    'Checkin service',\n",
    "    'Food and drink',\n",
    "    'Ease of Online booking',\n",
    "    'Age',\n",
    "    'service_score',\n",
    "    'Travel_Class',\n",
    "    'Is_New_Customer',\n",
    "    'Flight Category'\n",
    "]\n",
    "\n",
    "# به‌علاوه ستون‌های دسته‌ای که باید تبدیل بشن:\n",
    "categorical_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class']\n",
    "\n",
    "# فقط ستون‌های موردنیاز رو برمی‌داریم\n",
    "train = train[useful_features + categorical_cols + ['satisfaction']]\n",
    "test = test[useful_features + categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d448fd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['satisfaction'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "da5099f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add target back to the feature DataFrame temporarily\n",
    "df_corr = train.copy()\n",
    "df_corr['satisfaction_encoded'] = LabelEncoder().fit_transform(df_corr['satisfaction'])\n",
    "\n",
    "# Get numeric correlation\n",
    "corr = df_corr.corr(numeric_only=True)\n",
    "\n",
    "# Correlation with the target\n",
    "target_corr = corr['satisfaction_encoded'].sort_values(ascending=False)\n",
    "\n",
    "# Display the most and least related features\n",
    "print(target_corr)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=target_corr.values, y=target_corr.index)\n",
    "plt.title(\"Feature Correlation with Satisfaction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "78128618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Feature Engineering\n",
    "\n",
    "# normilzation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "num_cols = [\n",
    "    'Age', 'Flight Distance',\n",
    "    'Departure Delay in Minutes',\n",
    "    'Arrival Delay in Minutes'\n",
    "]\n",
    "\n",
    "# مقیاس‌دهی 0 تا 1\n",
    "scaler = MinMaxScaler()\n",
    "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "test[num_cols] = scaler.transform(test[num_cols])\n",
    "# # 1- Service Quality Score\n",
    "# train['service_score'] = (\n",
    "#     train['Inflight entertainment'] +\n",
    "#     train['Seat comfort'] +\n",
    "#     train['On-board service'] +\n",
    "#     train['Leg room service'] +\n",
    "#     train['Cleanliness'] +\n",
    "#     train['Inflight wifi service'] +\n",
    "#     train['Inflight service'] +\n",
    "#     train['Checkin service'] +\n",
    "#     train['Food and drink']\n",
    "# )\n",
    "# test['service_score'] = (\n",
    "#     test['Inflight entertainment'] +\n",
    "#     test['Seat comfort'] +\n",
    "#     test['On-board service'] +\n",
    "#     test['Leg room service'] +\n",
    "#     test['Cleanliness'] +\n",
    "#     test['Inflight wifi service'] +\n",
    "#     test['Inflight service'] +\n",
    "#     test['Checkin service'] +\n",
    "#     test['Food and drink']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd22b699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add target back to the feature DataFrame temporarily\n",
    "df_corr = train.copy()\n",
    "df_corr['satisfaction_encoded'] = LabelEncoder().fit_transform(df_corr['satisfaction'])\n",
    "\n",
    "# Get numeric correlation\n",
    "corr = df_corr.corr(numeric_only=True)\n",
    "\n",
    "# Correlation with the target\n",
    "target_corr = corr['satisfaction_encoded'].sort_values(ascending=False)\n",
    "\n",
    "# Display the most and least related features\n",
    "print(target_corr)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=target_corr.values, y=target_corr.index)\n",
    "plt.title(\"Feature Correlation with Satisfaction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d4c0847",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns = train.drop('satisfaction', axis=1).select_dtypes(include=['object', 'category']).columns\n",
    "print(dummy_columns)\n",
    "\n",
    "\n",
    "train = pd.get_dummies(train, columns=dummy_columns,dtype=np.uint8)\n",
    "test = pd.get_dummies(test, columns=dummy_columns,dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10900256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make Validation Set (Optional)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train.drop('satisfaction', axis=1),  # drop the column by name\n",
    "    train['satisfaction'],               # target variable\n",
    "    test_size=0.3,\n",
    "    random_state=42                     # optional: for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "028e4653",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Step 5: Define the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "L2_REG = 1e-5\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Reduced learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.95, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=150,  # Increased epochs\n",
    "    batch_size=32,  # Reduced batch size\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06c84f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras --upgrade\n",
    "# uncomment the above line if you don't have keras 3 installed\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5c7398ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv',engine='pyarrow')\n",
    "test = pd.read_csv('test.csv',engine ='pyarrow')\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f95631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train= train.drop(['Unnamed: 0','id'],axis=1)\n",
    "test= test.drop(['Unnamed: 0','id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20353be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.isna().any()\n",
    "train['Arrival Delay in Minutes'].fillna(train['Arrival Delay in Minutes'].mean(), inplace=True)\n",
    "\n",
    "test['Arrival Delay in Minutes'].fillna(test['Arrival Delay in Minutes'].mean(), inplace=True)\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1eb61303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Feature Engineering\n",
    "\n",
    "# normilzation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "num_cols = [\n",
    "    'Age', 'Flight Distance',\n",
    "    'Departure Delay in Minutes',\n",
    "    'Arrival Delay in Minutes'\n",
    "]\n",
    "\n",
    "# مقیاس‌دهی 0 تا 1\n",
    "scaler = MinMaxScaler()\n",
    "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "test[num_cols] = scaler.transform(test[num_cols])\n",
    "# # 1- Service Quality Score\n",
    "# train['service_score'] = (\n",
    "#     train['Inflight entertainment'] +\n",
    "#     train['Seat comfort'] +\n",
    "#     train['On-board service'] +\n",
    "#     train['Leg room service'] +\n",
    "#     train['Cleanliness'] +\n",
    "#     train['Inflight wifi service'] +\n",
    "#     train['Inflight service'] +\n",
    "#     train['Checkin service'] +\n",
    "#     train['Food and drink']\n",
    "# )\n",
    "# test['service_score'] = (\n",
    "#     test['Inflight entertainment'] +\n",
    "#     test['Seat comfort'] +\n",
    "#     test['On-board service'] +\n",
    "#     test['Leg room service'] +\n",
    "#     test['Cleanliness'] +\n",
    "#     test['Inflight wifi service'] +\n",
    "#     test['Inflight service'] +\n",
    "#     test['Checkin service'] +\n",
    "#     test['Food and drink']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "692fc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['satisfaction'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5cc130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add target back to the feature DataFrame temporarily\n",
    "df_corr = train.copy()\n",
    "df_corr['satisfaction_encoded'] = LabelEncoder().fit_transform(df_corr['satisfaction'])\n",
    "\n",
    "# Get numeric correlation\n",
    "corr = df_corr.corr(numeric_only=True)\n",
    "\n",
    "# Correlation with the target\n",
    "target_corr = corr['satisfaction_encoded'].sort_values(ascending=False)\n",
    "\n",
    "# Display the most and least related features\n",
    "print(target_corr)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=target_corr.values, y=target_corr.index)\n",
    "plt.title(\"Feature Correlation with Satisfaction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "93e76203",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns = train.drop('satisfaction', axis=1).select_dtypes(include=['object', 'category']).columns\n",
    "print(dummy_columns)\n",
    "\n",
    "\n",
    "train = pd.get_dummies(train, columns=dummy_columns,dtype=np.uint8)\n",
    "test = pd.get_dummies(test, columns=dummy_columns,dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9c3b27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make Validation Set (Optional)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train.drop('satisfaction', axis=1),  # drop the column by name\n",
    "    train['satisfaction'],               # target variable\n",
    "    test_size=0.3,\n",
    "    random_state=42                     # optional: for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "908d13b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make Validation Set (Optional)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train.drop('satisfaction', axis=1),  # drop the column by name\n",
    "    train['satisfaction'],               # target variable\n",
    "    test_size=0.3,\n",
    "    random_state=42                     # optional: for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cc32b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "90646a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b5e44f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Step 5: Define the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "L2_REG = 1e-5\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Reduced learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.95, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=150,  # Increased epochs\n",
    "    batch_size=32,  # Reduced batch size\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "86f9289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Step 5: Define the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "L2_REG = 1e-5\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Reduced learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.95, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=150,  # Increased epochs\n",
    "    batch_size=32,  # Reduced batch size\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fcfed4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Step 5: Define the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "L2_REG = 1e-5\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Reduced learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.95, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=150,  # Increased epochs\n",
    "    batch_size=32,  # Reduced batch size\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "17834695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make Validation Set (Optional)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train.drop('satisfaction', axis=1),  # drop the column by name\n",
    "    train['satisfaction'],               # target variable\n",
    "    test_size=0.3,\n",
    "    random_state=42                     # optional: for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "12ef05a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Step 5: Define the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "L2_REG = 1e-5\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Reduced learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.95, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=150,  # Increased epochs\n",
    "    batch_size=32,  # Reduced batch size\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fafb7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "\n",
    "# فقط ستون‌های مؤثر رو نگه می‌داریم:\n",
    "useful_features = [\n",
    "    'Online boarding',\n",
    "    'Inflight entertainment',\n",
    "    'Seat comfort',\n",
    "    'On-board service',\n",
    "    'Leg room service',\n",
    "    'Cleanliness',\n",
    "    'Flight Distance',\n",
    "    'Inflight wifi service',\n",
    "    'Baggage handling',\n",
    "    'Inflight service',\n",
    "    'Checkin service',\n",
    "    'Food and drink',\n",
    "    'Ease of Online booking',\n",
    "    'Age',\n",
    "    'service_score',\n",
    "    'Travel_Class',\n",
    "    'Is_New_Customer',\n",
    "    'Flight Category'\n",
    "]\n",
    "\n",
    "# به‌علاوه ستون‌های دسته‌ای که باید تبدیل بشن:\n",
    "categorical_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class']\n",
    "\n",
    "# فقط ستون‌های موردنیاز رو برمی‌داریم\n",
    "train = train[useful_features + categorical_cols + ['satisfaction']]\n",
    "test = test[useful_features + categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d3f38d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras --upgrade\n",
    "# uncomment the above line if you don't have keras 3 installed\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "83f18b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv',engine='pyarrow')\n",
    "test = pd.read_csv('test.csv',engine ='pyarrow')\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "32a68ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train= train.drop(['Unnamed: 0','id'],axis=1)\n",
    "test= test.drop(['Unnamed: 0','id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "43767158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.isna().any()\n",
    "train['Arrival Delay in Minutes'].fillna(train['Arrival Delay in Minutes'].mean(), inplace=True)\n",
    "\n",
    "test['Arrival Delay in Minutes'].fillna(test['Arrival Delay in Minutes'].mean(), inplace=True)\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "65f2fc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Feature Engineering\n",
    "\n",
    "# normilzation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "num_cols = [\n",
    "    'Age', 'Flight Distance',\n",
    "    'Departure Delay in Minutes',\n",
    "    'Arrival Delay in Minutes'\n",
    "]\n",
    "\n",
    "# مقیاس‌دهی 0 تا 1\n",
    "scaler = MinMaxScaler()\n",
    "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "test[num_cols] = scaler.transform(test[num_cols])\n",
    "# # 1- Service Quality Score\n",
    "# train['service_score'] = (\n",
    "#     train['Inflight entertainment'] +\n",
    "#     train['Seat comfort'] +\n",
    "#     train['On-board service'] +\n",
    "#     train['Leg room service'] +\n",
    "#     train['Cleanliness'] +\n",
    "#     train['Inflight wifi service'] +\n",
    "#     train['Inflight service'] +\n",
    "#     train['Checkin service'] +\n",
    "#     train['Food and drink']\n",
    "# )\n",
    "# test['service_score'] = (\n",
    "#     test['Inflight entertainment'] +\n",
    "#     test['Seat comfort'] +\n",
    "#     test['On-board service'] +\n",
    "#     test['Leg room service'] +\n",
    "#     test['Cleanliness'] +\n",
    "#     test['Inflight wifi service'] +\n",
    "#     test['Inflight service'] +\n",
    "#     test['Checkin service'] +\n",
    "#     test['Food and drink']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "00e34189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "\n",
    "# فقط ستون‌های مؤثر رو نگه می‌داریم:\n",
    "useful_features = [\n",
    "    'Online boarding',\n",
    "    'Inflight entertainment',\n",
    "    'Seat comfort',\n",
    "    'On-board service',\n",
    "    'Leg room service',\n",
    "    'Cleanliness',\n",
    "    'Flight Distance',\n",
    "    'Inflight wifi service',\n",
    "    'Baggage handling',\n",
    "    'Inflight service',\n",
    "    'Checkin service',\n",
    "    'Food and drink',\n",
    "    'Ease of Online booking',\n",
    "    'Age',\n",
    "    'service_score',\n",
    "    'Travel_Class',\n",
    "    'Is_New_Customer',\n",
    "    'Flight Category'\n",
    "]\n",
    "\n",
    "# به‌علاوه ستون‌های دسته‌ای که باید تبدیل بشن:\n",
    "categorical_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class']\n",
    "\n",
    "# فقط ستون‌های موردنیاز رو برمی‌داریم\n",
    "train = train[useful_features + categorical_cols + ['satisfaction']]\n",
    "test = test[useful_features + categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0c89d1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "\n",
    "# # فقط ستون‌های مؤثر رو نگه می‌داریم:\n",
    "# useful_features = [\n",
    "#     'Online boarding',\n",
    "#     'Inflight entertainment',\n",
    "#     'Seat comfort',\n",
    "#     'On-board service',\n",
    "#     'Leg room service',\n",
    "#     'Cleanliness',\n",
    "#     'Flight Distance',\n",
    "#     'Inflight wifi service',\n",
    "#     'Baggage handling',\n",
    "#     'Inflight service',\n",
    "#     'Checkin service',\n",
    "#     'Food and drink',\n",
    "#     'Ease of Online booking',\n",
    "#     'Age',\n",
    "#     'service_score',\n",
    "#     'Travel_Class',\n",
    "#     'Is_New_Customer',\n",
    "#     'Flight Category'\n",
    "# ]\n",
    "\n",
    "# # به‌علاوه ستون‌های دسته‌ای که باید تبدیل بشن:\n",
    "# categorical_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class']\n",
    "\n",
    "# # فقط ستون‌های موردنیاز رو برمی‌داریم\n",
    "# train = train[useful_features + categorical_cols + ['satisfaction']]\n",
    "# test = test[useful_features + categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f616bee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['satisfaction'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fc985ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add target back to the feature DataFrame temporarily\n",
    "df_corr = train.copy()\n",
    "df_corr['satisfaction_encoded'] = LabelEncoder().fit_transform(df_corr['satisfaction'])\n",
    "\n",
    "# Get numeric correlation\n",
    "corr = df_corr.corr(numeric_only=True)\n",
    "\n",
    "# Correlation with the target\n",
    "target_corr = corr['satisfaction_encoded'].sort_values(ascending=False)\n",
    "\n",
    "# Display the most and least related features\n",
    "print(target_corr)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=target_corr.values, y=target_corr.index)\n",
    "plt.title(\"Feature Correlation with Satisfaction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7a527c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns = train.drop('satisfaction', axis=1).select_dtypes(include=['object', 'category']).columns\n",
    "print(dummy_columns)\n",
    "\n",
    "\n",
    "train = pd.get_dummies(train, columns=dummy_columns,dtype=np.uint8)\n",
    "test = pd.get_dummies(test, columns=dummy_columns,dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "66602ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make Validation Set (Optional)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train.drop('satisfaction', axis=1),  # drop the column by name\n",
    "    train['satisfaction'],               # target variable\n",
    "    test_size=0.3,\n",
    "    random_state=42                     # optional: for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bb075b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a703d28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "80089e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Step 5: Define the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "L2_REG = 1e-5\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Reduced learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.95, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=150,  # Increased epochs\n",
    "    batch_size=32,  # Reduced batch size\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1afafc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Feature Engineering\n",
    "\n",
    "# normilzation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "num_cols = [\n",
    "    'Age', 'Flight Distance',\n",
    "    'Departure Delay in Minutes',\n",
    "    'Arrival Delay in Minutes'\n",
    "]\n",
    "\n",
    "# مقیاس‌دهی 0 تا 1\n",
    "scaler = MinMaxScaler()\n",
    "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "test[num_cols] = scaler.transform(test[num_cols])\n",
    "# 1- Service Quality Score\n",
    "train['service_score'] = (\n",
    "    train['Inflight entertainment'] +\n",
    "    train['Seat comfort'] +\n",
    "    train['On-board service'] +\n",
    "    train['Leg room service'] +\n",
    "    train['Cleanliness'] +\n",
    "    train['Inflight wifi service'] +\n",
    "    train['Inflight service'] +\n",
    "    train['Checkin service'] +\n",
    "    train['Food and drink']\n",
    ")\n",
    "test['service_score'] = (\n",
    "    test['Inflight entertainment'] +\n",
    "    test['Seat comfort'] +\n",
    "    test['On-board service'] +\n",
    "    test['Leg room service'] +\n",
    "    test['Cleanliness'] +\n",
    "    test['Inflight wifi service'] +\n",
    "    test['Inflight service'] +\n",
    "    test['Checkin service'] +\n",
    "    test['Food and drink']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "226975c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-Total Delay\n",
    "train['Total Delay'] = train['Departure Delay in Minutes'] + train['Arrival Delay in Minutes']\n",
    "train['Delay Category'] = pd.cut(train['Total Delay'], bins=[-1, 0, 15, 60, 180, float('inf')],\n",
    "                               labels=['No delay', 'Short', 'Moderate', 'Long', 'Extreme'])\n",
    "\n",
    "#2-Total Delay\n",
    "test['Total Delay'] = test['Departure Delay in Minutes'] + test['Arrival Delay in Minutes']\n",
    "test['Delay Category'] = pd.cut(test['Total Delay'], bins=[-1, 0, 15, 60, 180, float('inf')],\n",
    "                               labels=['No delay', 'Short', 'Moderate', 'Long', 'Extreme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "94587499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3- Travel Type\n",
    "train['Travel_Class'] = train['Type of Travel']  + train['Class']\n",
    "test['Travel_Class'] = test['Type of Travel']  + test['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f477c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-Total Delay\n",
    "train['Total Delay'] = train['Departure Delay in Minutes'] + train['Arrival Delay in Minutes']\n",
    "train['Delay Category'] = pd.cut(train['Total Delay'], bins=[-1, 0, 15, 60, 180, float('inf')],\n",
    "                               labels=['No delay', 'Short', 'Moderate', 'Long', 'Extreme'])\n",
    "\n",
    "#2-Total Delay\n",
    "test['Total Delay'] = test['Departure Delay in Minutes'] + test['Arrival Delay in Minutes']\n",
    "test['Delay Category'] = pd.cut(test['Total Delay'], bins=[-1, 0, 15, 60, 180, float('inf')],\n",
    "                               labels=['No delay', 'Short', 'Moderate', 'Long', 'Extreme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9e1e0df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv',engine='pyarrow')\n",
    "test = pd.read_csv('test.csv',engine ='pyarrow')\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f198f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train= train.drop(['Unnamed: 0','id'],axis=1)\n",
    "test= test.drop(['Unnamed: 0','id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fc6a768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.isna().any()\n",
    "train['Arrival Delay in Minutes'].fillna(train['Arrival Delay in Minutes'].mean(), inplace=True)\n",
    "\n",
    "test['Arrival Delay in Minutes'].fillna(test['Arrival Delay in Minutes'].mean(), inplace=True)\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "70df11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Feature Engineering\n",
    "\n",
    "# normilzation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "num_cols = [\n",
    "    'Age', 'Flight Distance',\n",
    "    'Departure Delay in Minutes',\n",
    "    'Arrival Delay in Minutes'\n",
    "]\n",
    "\n",
    "# مقیاس‌دهی 0 تا 1\n",
    "scaler = MinMaxScaler()\n",
    "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "test[num_cols] = scaler.transform(test[num_cols])\n",
    "# 1- Service Quality Score\n",
    "train['service_score'] = (\n",
    "    train['Inflight entertainment'] +\n",
    "    train['Seat comfort'] +\n",
    "    train['On-board service'] +\n",
    "    train['Leg room service'] +\n",
    "    train['Cleanliness'] +\n",
    "    train['Inflight wifi service'] +\n",
    "    train['Inflight service'] +\n",
    "    train['Checkin service'] +\n",
    "    train['Food and drink']\n",
    ")\n",
    "test['service_score'] = (\n",
    "    test['Inflight entertainment'] +\n",
    "    test['Seat comfort'] +\n",
    "    test['On-board service'] +\n",
    "    test['Leg room service'] +\n",
    "    test['Cleanliness'] +\n",
    "    test['Inflight wifi service'] +\n",
    "    test['Inflight service'] +\n",
    "    test['Checkin service'] +\n",
    "    test['Food and drink']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c6fefbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-Total Delay\n",
    "train['Total Delay'] = train['Departure Delay in Minutes'] + train['Arrival Delay in Minutes']\n",
    "train['Delay Category'] = pd.cut(train['Total Delay'], bins=[-1, 0, 15, 60, 180, float('inf')],\n",
    "                               labels=['No delay', 'Short', 'Moderate', 'Long', 'Extreme'])\n",
    "\n",
    "#2-Total Delay\n",
    "test['Total Delay'] = test['Departure Delay in Minutes'] + test['Arrival Delay in Minutes']\n",
    "test['Delay Category'] = pd.cut(test['Total Delay'], bins=[-1, 0, 15, 60, 180, float('inf')],\n",
    "                               labels=['No delay', 'Short', 'Moderate', 'Long', 'Extreme'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1033aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3- Travel Type\n",
    "train['Travel_Class'] = train['Type of Travel']  + train['Class']\n",
    "test['Travel_Class'] = test['Type of Travel']  + test['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4d8ebd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4- Loyalty or New Customer\n",
    "train['Is_New_Customer'] = (train['Customer Type'] == 'disloyal Customer').astype(int)\n",
    "test['Is_New_Customer'] = (test['Customer Type'] == 'disloyal Customer').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1aa46423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5- Flight Duration\n",
    "train['Flight Category'] = pd.cut(train['Flight Distance'], bins=[0, 500, 1500, 3000, float('inf')],\n",
    "                               labels=['Short', 'Medium', 'Long', 'Very Long'])\n",
    "test['Flight Category'] = pd.cut(test['Flight Distance'], bins=[0, 500, 1500, 3000, float('inf')],\n",
    "                               labels=['Short', 'Medium', 'Long', 'Very Long'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "78dfabc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "\n",
    "# # فقط ستون‌های مؤثر رو نگه می‌داریم:\n",
    "# useful_features = [\n",
    "#     'Online boarding',\n",
    "#     'Inflight entertainment',\n",
    "#     'Seat comfort',\n",
    "#     'On-board service',\n",
    "#     'Leg room service',\n",
    "#     'Cleanliness',\n",
    "#     'Flight Distance',\n",
    "#     'Inflight wifi service',\n",
    "#     'Baggage handling',\n",
    "#     'Inflight service',\n",
    "#     'Checkin service',\n",
    "#     'Food and drink',\n",
    "#     'Ease of Online booking',\n",
    "#     'Age',\n",
    "#     'service_score',\n",
    "#     'Travel_Class',\n",
    "#     'Is_New_Customer',\n",
    "#     'Flight Category'\n",
    "# ]\n",
    "\n",
    "# # به‌علاوه ستون‌های دسته‌ای که باید تبدیل بشن:\n",
    "# categorical_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class']\n",
    "\n",
    "# # فقط ستون‌های موردنیاز رو برمی‌داریم\n",
    "# train = train[useful_features + categorical_cols + ['satisfaction']]\n",
    "# test = test[useful_features + categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a0788834",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['satisfaction'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5264087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add target back to the feature DataFrame temporarily\n",
    "df_corr = train.copy()\n",
    "df_corr['satisfaction_encoded'] = LabelEncoder().fit_transform(df_corr['satisfaction'])\n",
    "\n",
    "# Get numeric correlation\n",
    "corr = df_corr.corr(numeric_only=True)\n",
    "\n",
    "# Correlation with the target\n",
    "target_corr = corr['satisfaction_encoded'].sort_values(ascending=False)\n",
    "\n",
    "# Display the most and least related features\n",
    "print(target_corr)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=target_corr.values, y=target_corr.index)\n",
    "plt.title(\"Feature Correlation with Satisfaction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d24ba860",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns = train.drop('satisfaction', axis=1).select_dtypes(include=['object', 'category']).columns\n",
    "print(dummy_columns)\n",
    "\n",
    "\n",
    "train = pd.get_dummies(train, columns=dummy_columns,dtype=np.uint8)\n",
    "test = pd.get_dummies(test, columns=dummy_columns,dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9657b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make Validation Set (Optional)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train.drop('satisfaction', axis=1),  # drop the column by name\n",
    "    train['satisfaction'],               # target variable\n",
    "    test_size=0.3,\n",
    "    random_state=42                     # optional: for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f4927299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Step 5: Define the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "L2_REG = 1e-5\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Reduced learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.95, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=150,  # Increased epochs\n",
    "    batch_size=32,  # Reduced batch size\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "83a5b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Step 5: Define the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "L2_REG = 1e-5\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Reduced learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.95, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Ensure all columns in X_train and X_valid are numeric\n",
    "X_train = X_train.select_dtypes(include=np.number)\n",
    "X_valid = X_valid.select_dtypes(include=np.number)\n",
    "\n",
    "# If y_train is not numeric (e.g., 'satisfied'/'dissatisfied'), encode it:\n",
    "if y_train.dtype == 'object':\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    y_valid = label_encoder.transform(y_valid)  # Use the same encoder for validation\n",
    "\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=150,  # Increased epochs\n",
    "    batch_size=32,  # Reduced batch size\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "69968b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv',engine='pyarrow')\n",
    "test = pd.read_csv('test.csv',engine ='pyarrow')\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "776dbeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train= train.drop(['Unnamed: 0','id'],axis=1)\n",
    "test= test.drop(['Unnamed: 0','id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4d1ecc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.isna().any()\n",
    "train['Arrival Delay in Minutes'].fillna(train['Arrival Delay in Minutes'].mean(), inplace=True)\n",
    "\n",
    "test['Arrival Delay in Minutes'].fillna(test['Arrival Delay in Minutes'].mean(), inplace=True)\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e17a65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Feature Engineering\n",
    "\n",
    "# normilzation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "num_cols = [\n",
    "    'Age', 'Flight Distance',\n",
    "    'Departure Delay in Minutes',\n",
    "    'Arrival Delay in Minutes'\n",
    "]\n",
    "\n",
    "# مقیاس‌دهی 0 تا 1\n",
    "scaler = MinMaxScaler()\n",
    "train[num_cols] = scaler.fit_transform(train[num_cols])\n",
    "test[num_cols] = scaler.transform(test[num_cols])\n",
    "# # 1- Service Quality Score\n",
    "# train['service_score'] = (\n",
    "#     train['Inflight entertainment'] +\n",
    "#     train['Seat comfort'] +\n",
    "#     train['On-board service'] +\n",
    "#     train['Leg room service'] +\n",
    "#     train['Cleanliness'] +\n",
    "#     train['Inflight wifi service'] +\n",
    "#     train['Inflight service'] +\n",
    "#     train['Checkin service'] +\n",
    "#     train['Food and drink']\n",
    "# )\n",
    "# test['service_score'] = (\n",
    "#     test['Inflight entertainment'] +\n",
    "#     test['Seat comfort'] +\n",
    "#     test['On-board service'] +\n",
    "#     test['Leg room service'] +\n",
    "#     test['Cleanliness'] +\n",
    "#     test['Inflight wifi service'] +\n",
    "#     test['Inflight service'] +\n",
    "#     test['Checkin service'] +\n",
    "#     test['Food and drink']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9cbad0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from keras import regularizers\n",
    "\n",
    "# # فقط ستون‌های مؤثر رو نگه می‌داریم:\n",
    "# useful_features = [\n",
    "#     'Online boarding',\n",
    "#     'Inflight entertainment',\n",
    "#     'Seat comfort',\n",
    "#     'On-board service',\n",
    "#     'Leg room service',\n",
    "#     'Cleanliness',\n",
    "#     'Flight Distance',\n",
    "#     'Inflight wifi service',\n",
    "#     'Baggage handling',\n",
    "#     'Inflight service',\n",
    "#     'Checkin service',\n",
    "#     'Food and drink',\n",
    "#     'Ease of Online booking',\n",
    "#     'Age',\n",
    "#     'service_score',\n",
    "#     'Travel_Class',\n",
    "#     'Is_New_Customer',\n",
    "#     'Flight Category'\n",
    "# ]\n",
    "\n",
    "# # به‌علاوه ستون‌های دسته‌ای که باید تبدیل بشن:\n",
    "# categorical_cols = ['Gender', 'Customer Type', 'Type of Travel', 'Class']\n",
    "\n",
    "# # فقط ستون‌های موردنیاز رو برمی‌داریم\n",
    "# train = train[useful_features + categorical_cols + ['satisfaction']]\n",
    "# test = test[useful_features + categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2950cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add target back to the feature DataFrame temporarily\n",
    "df_corr = train.copy()\n",
    "df_corr['satisfaction_encoded'] = LabelEncoder().fit_transform(df_corr['satisfaction'])\n",
    "\n",
    "# Get numeric correlation\n",
    "corr = df_corr.corr(numeric_only=True)\n",
    "\n",
    "# Correlation with the target\n",
    "target_corr = corr['satisfaction_encoded'].sort_values(ascending=False)\n",
    "\n",
    "# Display the most and least related features\n",
    "print(target_corr)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=target_corr.values, y=target_corr.index)\n",
    "plt.title(\"Feature Correlation with Satisfaction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2c2044a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_columns = train.drop('satisfaction', axis=1).select_dtypes(include=['object', 'category']).columns\n",
    "print(dummy_columns)\n",
    "\n",
    "\n",
    "train = pd.get_dummies(train, columns=dummy_columns,dtype=np.uint8)\n",
    "test = pd.get_dummies(test, columns=dummy_columns,dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "41e009e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make Validation Set (Optional)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train.drop('satisfaction', axis=1),  # drop the column by name\n",
    "    train['satisfaction'],               # target variable\n",
    "    test_size=0.3,\n",
    "    random_state=42                     # optional: for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f5f0d564",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "\n",
    "# Step 5: Define the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "L2_REG = 1e-5\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal', kernel_regularizer=regularizers.L1L2(L2_REG))(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),  # Reduced learning rate\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.95, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Ensure all columns in X_train and X_valid are numeric\n",
    "X_train = X_train.select_dtypes(include=np.number)\n",
    "X_valid = X_valid.select_dtypes(include=np.number)\n",
    "\n",
    "# If y_train is not numeric (e.g., 'satisfied'/'dissatisfied'), encode it:\n",
    "if y_train.dtype == 'object':\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(y_train)\n",
    "    y_valid = label_encoder.transform(y_valid)  # Use the same encoder for validation\n",
    "\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=150,  # Increased epochs\n",
    "    batch_size=32,  # Reduced batch size\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "804f324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Step 1: Encode categorical features\n",
    "X = pd.get_dummies(train.drop(columns=['satisfaction']), drop_first=True)\n",
    "\n",
    "# Step 2: Encode target\n",
    "y = LabelEncoder().fit_transform(train['satisfaction'])  # 0: neutral/dissatisfied, 1: satisfied\n",
    "\n",
    "# Step 3: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 4: Split the data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 5: Define the model\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "x = Dense(256, kernel_initializer='he_normal')(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(128, kernel_initializer='he_normal')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "x = Dense(64, kernel_initializer='he_normal')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "97d74bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Step 1: Encode categorical features using one-hot encoding (drop_first to avoid dummy trap)\n",
    "X = pd.get_dummies(train.drop(columns=['satisfaction']), drop_first=True)\n",
    "\n",
    "# Step 2: Encode target variable\n",
    "y = LabelEncoder().fit_transform(train['satisfaction'])  # 0: neutral/dissatisfied, 1: satisfied\n",
    "\n",
    "# Step 3: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 4: Split the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 5: Define the model with best hyperparameters\n",
    "inputs = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# First Dense block: 320 neurons with dropout 0.3\n",
    "x = Dense(320, kernel_initializer='he_normal')(inputs)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Second Dense block: 160 neurons with dropout 0.3\n",
    "x = Dense(160, kernel_initializer='he_normal')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Third Dense block: 96 neurons with dropout 0.1\n",
    "x = Dense(96, kernel_initializer='he_normal')(x)\n",
    "x = LeakyReLU()(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# Output layer for binary classification\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Step 6: Compile model using the specified learning rate (0.001)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Step 7: Define callbacks\n",
    "checkpoint_cb = ModelCheckpoint(\"best_model.keras\", save_best_only=True)\n",
    "earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "reduce_lr_cb = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=1e-5, verbose=1)\n",
    "\n",
    "# Step 8: Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[checkpoint_cb, earlystop_cb, reduce_lr_cb],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a1b15679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred_probs = model.predict(X_valid)\n",
    "y_val_pred = (y_val_pred_probs > 0.5).astype(int).flatten()\n",
    "accuracy = accuracy_score(y_valid, y_val_pred)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predict on the test set (assumes 'test' is preprocessed identically to the train set)\n",
    "y_test_pred_probs = model.predict(test)\n",
    "y_test_pred = (y_test_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Map predicted classes (0/1) to labels\n",
    "label_map = {0: 'neutral or dissatisfied', 1: 'satisfied'}\n",
    "y_test_labels = [label_map[val] for val in y_test_pred]\n",
    "print(\"Test set predictions:\")\n",
    "print(y_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cf0c5b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['satisfaction'] = y_test_labels\n",
    "print(submission)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
